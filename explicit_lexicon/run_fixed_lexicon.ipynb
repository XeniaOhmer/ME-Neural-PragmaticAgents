{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from agents import PragmaticListener, PragmaticSpeaker, LiteralListener\n",
    "from data_helpers import generate_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE AGENT AND SAVE PARAMETERS + RESULTS\n",
    "\n",
    "def run_agent(n=100, \n",
    "              reasoning=1, # 0 = literal learning, 1=pragmatic learning\n",
    "              n_epochs=1000, \n",
    "              input_dist='Zipf',\n",
    "              learning_rate=0.1,\n",
    "              alpha=5.,\n",
    "              iter_step=10,\n",
    "              train_mode='RL', \n",
    "              batch_size = 32, \n",
    "              data_size = 1000,\n",
    "              init_mean = 0.01,\n",
    "              runs = 100, \n",
    "              samples = 25,\n",
    "              inference = 'pragmatic' # 'literal'=literal inference, 'pragmatic'=pragmatic inference\n",
    "            ):\n",
    "    \n",
    "    ### setup the training and save the parameters ###\n",
    "\n",
    "    batches = data_size // batch_size  # number of bathes per epoch\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "    if inference == pragmatic and reasoning == 1: \n",
    "        filename = ('results_fixed_lexicon/interval_' + str(iter_step) + '_alpha_' + str(alpha) + '/')\n",
    "    else: \n",
    "        filename = ('results_fixed_lexicon/' + ['literal', 'pragmatic'][reasoning] + '-learning_' +\n",
    "                    inference + '-inference/interval_' + str(iter_step) + '_alpha_' + str(alpha) + '/')\n",
    "              \n",
    "    if not os.path.exists(filename):\n",
    "            os.makedirs(filename)\n",
    "            \n",
    "    param_dict = {\"n_states\": n,\n",
    "                  \"n_messages\": n, \n",
    "                  \"n_epochs\": n_epochs, \n",
    "                  \"batch_size\": batch_size,\n",
    "                  \"data_size\": data_size, \n",
    "                  \"init_mean\": init_mean,\n",
    "                  \"learning_rate\": learning_rate, \n",
    "                  \"runs\": runs, \n",
    "                  \"alpha\": alpha,\n",
    "                  \"iter_step\": iter_step,\n",
    "                  \"reasoning\": reasoning,\n",
    "                  \"input_dist\": input_dist, \n",
    "                  \"train_mode\": train_mode,\n",
    "                  \"agent_mode\": \"single\", \n",
    "                  \"samples\": samples, \n",
    "                  \"inference\": inference\n",
    "                 }\n",
    "    with open(filename + 'param_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(param_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    ### iterate over runs ###\n",
    "    \n",
    "    for run in range(runs):\n",
    "        \n",
    "        print('interval: ', iter_step, ', run: ', run)\n",
    "        \n",
    "        counts = np.zeros((n_epochs, n, 3)) # 0: correct, 1: false , 2: potentially correct\n",
    "        \n",
    "        ### build the agent ###\n",
    "        \n",
    "        lexicon = tf.Variable(tf.initializers.Constant(init_mean)([n, n]),\n",
    "                              name=\"lexicon\", \n",
    "                              trainable=True, \n",
    "                              dtype=tf.float32,\n",
    "                              constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        if reasoning == 0:\n",
    "            listener = LiteralListener(n, n, lexicon)\n",
    "        elif reasoning == 1:\n",
    "            listener = PragmaticListener(n, n, lexicon, alpha=alpha)\n",
    "        listener.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "        \n",
    "        ### train the agent ###\n",
    "        \n",
    "        # save the following results:\n",
    "        # rewards per epoch --> all_rewards \n",
    "        # policies in the context-dependent evaluation per epoch --> policies_all_reference \n",
    "        # policies in the context-free evaluation per epoch --> policies_all_lewis\n",
    "        # number of correct, false, and potentially correct selections per epoch --> counts\n",
    "        # (potentially correct means any selecting any of the unfamiliar objects, not necessarily the correct one)\n",
    "        \n",
    "        all_rewards = []\n",
    "        all_policies_lewis = []\n",
    "        all_policies_reference = []\n",
    "        number_of_states = 0\n",
    "        has_occurred = np.zeros(n)\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            # add a new sample every iter_step \n",
    "            if i % iter_step == 0 and number_of_states<n: \n",
    "                number_of_states += 1  \n",
    "                \n",
    "                # lewis game evaluation = context-free evaluation\n",
    "                new_message = np.zeros((1, n, n))\n",
    "                new_message[:, number_of_states-1, :] = 1.\n",
    "                \n",
    "                if inference == 'pragmatic': \n",
    "                    pragmatic_listener = PragmaticListener(n, n, np.copy(listener.lexicon[:]), alpha=alpha)\n",
    "                    policy, states = pragmatic_listener.get_states(new_message)\n",
    "                elif inference == 'literal': \n",
    "                    literal_listener = LiteralListener(n, n, np.copy(listener.lexicon[:]))\n",
    "                    policy, states = literal_listener.get_states(new_message)\n",
    "                    \n",
    "                policy = policy / tf.expand_dims(tf.reduce_sum(policy, axis=1), axis=1)\n",
    "                all_policies_lewis.append(policy)\n",
    "                \n",
    "                # reference game evaluation = context-dependent evaluation\n",
    "                if number_of_states >= 2:\n",
    "                    policies_state  = np.empty(99)\n",
    "                    policies_state[:] = np.NaN\n",
    "                    \n",
    "                    old_lexicon = np.copy(listener.lexicon[:])[0:number_of_states-1, 0:number_of_states-1]\n",
    "                    current_lexicon = np.copy(listener.lexicon[:])[0:number_of_states, 0:number_of_states]\n",
    "                    \n",
    "                    for j in range(number_of_states-1):\n",
    "                        \n",
    "                        if inference == 'pragmatic':\n",
    "                            \n",
    "                            test_speaker = PragmaticSpeaker(number_of_states-1, number_of_states-1, old_lexicon, alpha=alpha)\n",
    "                            speaker_target = np.zeros((1, number_of_states-1, number_of_states-1), dtype=np.float32)\n",
    "                            speaker_target[0,j,:] = np.ones((1, number_of_states-1))\n",
    "                            policy_speaker, _ = test_speaker.get_messages(speaker_target)\n",
    "\n",
    "                        elif inference == 'literal':\n",
    "                            \n",
    "                            policy_speaker = [old_lexicon[j,:]]\n",
    "                        \n",
    "                        policy_speaker = policy_speaker / tf.reduce_sum(policy_speaker)\n",
    "                        relevant_messages = np.unique(np.random.choice(number_of_states-1, size=samples, \n",
    "                                                                           p=np.array(policy_speaker)[0]))\n",
    "                        relevant_messages = np.append(relevant_messages, number_of_states-1)\n",
    "                            \n",
    "                        dim2 = len(relevant_messages)\n",
    "                        test_lexicon = np.zeros((2, len(relevant_messages)))\n",
    "                        test_lexicon[0,:] = current_lexicon[j, relevant_messages]\n",
    "                        test_lexicon[1,:] = current_lexicon[number_of_states-1, relevant_messages]\n",
    "                        \n",
    "                        if inference == 'pragmatic':\n",
    "                            test_listener = PragmaticListener(2, len(relevant_messages)+1, test_lexicon, alpha=alpha)\n",
    "                            \n",
    "                        elif inference == 'literal':\n",
    "                            test_listener = LiteralListener(2, len(relevant_messages)+1, test_lexicon)\n",
    "                        \n",
    "                        target = np.zeros((1, dim2, 2))\n",
    "                        target[0, dim2-1, :] = 1\n",
    "                        p, _ = test_listener.get_states(target)\n",
    "                        p = p / tf.reduce_sum(p)\n",
    "                        policies_state[j] = np.array(p)[:,-1]\n",
    "                    \n",
    "                    all_policies_reference.append(policies_state) \n",
    "                \n",
    "            # generate random data set   \n",
    "            data, selections, labels = generate_inputs(data_size, \n",
    "                                                       n, \n",
    "                                                       number_of_states, \n",
    "                                                       distribution=input_dist)\n",
    "            shuffle_indices = np.random.permutation(data_size)\n",
    "            data = tf.gather(data, shuffle_indices)\n",
    "            labels = tf.gather(labels, shuffle_indices)\n",
    "            selections = tf.gather(selections, shuffle_indices)\n",
    "            \n",
    "            rewards_epoch = []\n",
    "            \n",
    "            # training loop\n",
    "            for j in range(batches):\n",
    "                data_batch = data[j:j+batch_size]\n",
    "                labels_batch = labels[j:j+batch_size]\n",
    "                selection_batch = np.array(selections[j:j+batch_size])\n",
    "        \n",
    "                _, states = listener.get_states(data_batch)\n",
    "                states_non_hot = np.argmax(states, axis=1)\n",
    "                rewards = tf.einsum('ij,ij->i', tf.cast(labels_batch, dtype=tf.float32), states)\n",
    "                \n",
    "                if train_mode == 'RL':\n",
    "                    loss = listener.train_on_batch(data_batch, states, sample_weight=rewards)\n",
    "                elif train_mode == 'supervised':\n",
    "                    loss = listener.train_on_batch(data_batch, labels_batch)\n",
    "    \n",
    "                rewards_epoch.append(np.mean(rewards))\n",
    "                \n",
    "                correct_states = selection_batch[states_non_hot==selection_batch]\n",
    "                false_states = selection_batch[np.logical_and(states_non_hot!=selection_batch, \n",
    "                                                              has_occurred[states_non_hot]==1)]\n",
    "                potentially_correct_states = selection_batch[np.logical_and(states_non_hot!=selection_batch, \n",
    "                                                                            has_occurred[states_non_hot]==0)]\n",
    "                unique, occurrences = np.unique(correct_states, return_counts=True)\n",
    "                counts[i, unique, 0] += occurrences \n",
    "                unique, occurrences = np.unique(false_states, return_counts=True)\n",
    "                counts[i, unique, 1] += occurrences \n",
    "                unique, occurrences = np.unique(potentially_correct_states, return_counts=True)\n",
    "                counts[i, unique, 2] += occurrences \n",
    "                \n",
    "                has_occurred[selection_batch] = 1\n",
    "            \n",
    "            mean_reward = np.mean(rewards_epoch)\n",
    "            all_rewards.append(mean_reward)        \n",
    "        \n",
    "        print('final reward ' + str(all_rewards[iter_step * n - 1]))\n",
    "        \n",
    "        # save results \n",
    "        np.save(filename + 'counts_' + str(run), counts)\n",
    "        np.save(filename + 'rewards_'+ str(run), all_rewards)\n",
    "        np.save(filename + 'policies_lewis_' + str(run), all_policies_lewis)\n",
    "        np.save(filename + 'policies_reference_' + str(run), all_policies_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main results \n",
    "\n",
    "for iter_step in [1,3,6,9,12,15]:\n",
    "    if iter_step == 1: \n",
    "        run_agent(iter_step=iter_step, n_epochs=iter_step*100 + 1000, runs=500)\n",
    "    else:\n",
    "        run_agent(iter_step=iter_step, n_epochs=iter_step*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different learning - inference combinations\n",
    "\n",
    "for iter_step in [1,3,6,9,12,15]:\n",
    "    for reasoning in [0,1]:\n",
    "        if reasoning == 0: \n",
    "            inference = 'pragmatic'\n",
    "        if reasoning == 1: \n",
    "            inference = 'literal'\n",
    "        if iter_step == 1: \n",
    "            run_agent(iter_step=iter_step, n_epochs=iter_step*100 + 1000, runs=500, inference=inference)\n",
    "        else:\n",
    "            run_agent(iter_step=iter_step, n_epochs=iter_step*100, inference=inference)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
