{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from agents import PragmaticListener, PragmaticSpeaker, LiteralListener\n",
    "from data_helpers import generate_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN THE AGENT AND SAVE PARAMETERS + RESULTS\n",
    "\n",
    "def run_agent(n=100, \n",
    "              reasoning=1, \n",
    "              n_epochs=1000, \n",
    "              input_dist='Zipf',\n",
    "              learning_rate=0.1,\n",
    "              alpha=5.,\n",
    "              iter_step=10,\n",
    "              train_mode='RL', \n",
    "              batch_size = 32, \n",
    "              data_size = 1000,\n",
    "              init_mean = 0.1,\n",
    "              runs = 100):\n",
    "    \n",
    "    ### setup the training and save the parameters ###\n",
    "\n",
    "    batches = data_size // batch_size  # number of bathes per epoch\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "    filename = ('results_dynamic_lexicon/interval_' + str(iter_step) \n",
    "                + '_alpha_' + str(alpha) + '/')\n",
    "    if not os.path.exists(filename):\n",
    "            os.makedirs(filename)\n",
    "            \n",
    "    param_dict = {\"n_states\": n,\n",
    "                  \"n_messages\": n, \n",
    "                  \"n_epochs\": n_epochs, \n",
    "                  \"batch_size\": batch_size,\n",
    "                  \"data_size\": data_size, \n",
    "                  \"init_mean\": init_mean,\n",
    "                  \"learning_rate\": learning_rate, \n",
    "                  \"runs\": runs, \n",
    "                  \"alpha\": alpha,\n",
    "                  \"iter_step\": iter_step,\n",
    "                  \"reasoning\": reasoning,\n",
    "                  \"input_dist\": input_dist, \n",
    "                  \"train_mode\": train_mode,\n",
    "                  \"agent_mode\": \"single\"}\n",
    "    with open(filename + 'param_dict.pickle', 'wb') as handle:\n",
    "        pickle.dump(param_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    ### iterate over runs ###\n",
    "    \n",
    "    n_start = 2 # initial lexicon size\n",
    "\n",
    "    for run in range(runs):\n",
    "        \n",
    "        print('interval: ', iter_step, ', run: ', run)\n",
    "        \n",
    "        ### build the agent ###\n",
    "        \n",
    "        lexicon = tf.Variable(tf.initializers.Constant(init_mean)\n",
    "                              ([n_start, n_start]),\n",
    "                              name=\"lexicon\", \n",
    "                              trainable=True, \n",
    "                              dtype=tf.float32,\n",
    "                              constraint=tf.keras.constraints.NonNeg())\n",
    "        \n",
    "        listener = PragmaticListener(n_start, n_start, lexicon, alpha=alpha)\n",
    "        listener.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "        \n",
    "        ### train the agent ###\n",
    "        \n",
    "        # save the following results: \n",
    "        # rewards per epoch --> all_rewards \n",
    "        # policies in the context-dependent evaluation per epoch --> policies_all_reference \n",
    "        # policies in the context-free evaluation per epoch --> policies_all_lewis\n",
    "        # number of correct, false, and potentially correct selections per epoch --> counts\n",
    "        # (potentially correct is not relevant here, only for the fixed lexicon)\n",
    "        \n",
    "        all_rewards = []                    \n",
    "        policies_all_reference = []\n",
    "        policies_all_lewis = []\n",
    "        number_of_states = n_start\n",
    "        has_occurred = np.zeros(n)\n",
    "        counts = np.zeros((n_epochs, n, 3)) # 0: correct, 1: false , 2: potentially correct\n",
    "        \n",
    "        for i in range(n_epochs):\n",
    "            \n",
    "            # add a new sample every iter_step and accordingly expand the lexicon\n",
    "            if i % iter_step == 0 and i>=n_start*iter_step and number_of_states < 100: \n",
    "                number_of_states += 1     \n",
    "                \n",
    "                # expand lexicon\n",
    "                old_lexicon = np.copy(listener.lexicon[:])\n",
    "                old_lexicon_mean = np.mean(old_lexicon)\n",
    "                new_lexicon = old_lexicon_mean * np.ones((number_of_states, number_of_states), \n",
    "                                                         dtype=np.float32)\n",
    "\n",
    "                new_lexicon[0:-1,0:-1] = old_lexicon\n",
    "                plt.imshow(new_lexicon)\n",
    "                plt.colorbar()\n",
    "                plt.show()\n",
    "                new_lexicon = tf.Variable(new_lexicon, dtype=tf.float32, constraint=tf.keras.constraints.NonNeg())\n",
    "                if reasoning == 1: \n",
    "                    listener = PragmaticListener(number_of_states, number_of_states, new_lexicon, alpha=alpha)\n",
    "                elif reasoning == 0:\n",
    "                    listener = LiteralListener(number_of_states, number_of_states, new_lexicon)\n",
    "                    \n",
    "                listener.compile(optimizer=optimizer, loss=tf.keras.losses.CategoricalCrossentropy())\n",
    "                \n",
    "                # lewis game evalution = context-free evaluation\n",
    "                new_message = np.zeros((1,number_of_states, number_of_states))\n",
    "                new_message[:,-1,:] = 1.\n",
    "                policy, states = listener.get_states(new_message)\n",
    "                policy = policy / tf.expand_dims(tf.reduce_sum(policy, axis=1), axis=1)\n",
    "                policies_all_lewis.append(policy)\n",
    "          \n",
    "                # reference game evaluation = context-dependent evaluation\n",
    "                policies_state  = np.empty(99)\n",
    "                policies_state[:] = np.NaN\n",
    "                for j in range(number_of_states-1):\n",
    "            \n",
    "                    relevant_row = old_lexicon[j,:]\n",
    "                    \n",
    "                    if reasoning == 1: \n",
    "                        test_speaker = PragmaticSpeaker(number_of_states-1, number_of_states-1, \n",
    "                                                   old_lexicon, alpha=alpha)\n",
    "                    elif reasoning == 0:\n",
    "                        test_speaker = PragmaticSpeaker(number_of_states-1, number_of_states-1, old_lexicon)\n",
    "                        \n",
    "                    speaker_target = np.zeros((1, number_of_states-1, number_of_states-1), dtype=np.float32)\n",
    "                    speaker_target[0,j,:] = np.ones((1, number_of_states-1))\n",
    "                    policy_speaker, _ = test_speaker.get_messages(speaker_target)\n",
    "                    policy_speaker = policy_speaker / tf.reduce_sum(policy_speaker)\n",
    "                    relevant_messages = np.unique(np.random.choice(number_of_states-1, size=25, p=np.array(policy_speaker)[0]))\n",
    "                    \n",
    "                    dim1 = 2\n",
    "                    dim2 = len(relevant_messages)+1\n",
    "                    test_lexicon = old_lexicon_mean * np.ones((dim1, dim2))\n",
    "                    test_lexicon[0,0:len(relevant_messages)] = relevant_row[relevant_messages]\n",
    "                    \n",
    "                    if reasoning == 1: \n",
    "                        test_listener = PragmaticListener(2, len(relevant_messages)+1, test_lexicon, alpha=alpha)\n",
    "                    elif reasoning == 0:\n",
    "                        test_listener = LiteralListener(2, len(relevant_messages)+1, test_lexicon)\n",
    "                        \n",
    "                    target = np.zeros((1, dim2, dim1))\n",
    "                    target[0, dim2-1, :] = 1\n",
    "                    p, _ = test_listener.get_states(target)\n",
    "                    p = p / tf.reduce_sum(p)\n",
    "                    policies_state[j] = np.array(p)[:,-1]\n",
    "                \n",
    "                policies_all_reference.append(policies_state)\n",
    "            \n",
    "            # generate random data set\n",
    "            data, selection, labels = generate_inputs(data_size, \n",
    "                                                      number_of_states, \n",
    "                                                      number_of_states, \n",
    "                                                      distribution=input_dist)\n",
    "            shuffle_indices = np.random.permutation(data_size)\n",
    "            data = tf.gather(data, shuffle_indices)\n",
    "            labels = tf.gather(labels, shuffle_indices)\n",
    "            selection = tf.gather(selection, shuffle_indices)\n",
    "            \n",
    "            rewards_epoch = []\n",
    "            \n",
    "            # training loop\n",
    "            for j in range(batches):\n",
    "                data_batch = data[j:j+batch_size]\n",
    "                labels_batch = labels[j:j+batch_size]\n",
    "                selection_batch = np.array(selection[j:j+batch_size])\n",
    "                \n",
    "                p, states = listener.get_states(data_batch)\n",
    "                rewards = tf.einsum('ij,ij->i', tf.cast(labels_batch, tf.float32), states)\n",
    "                if train_mode == 'RL':\n",
    "                        loss = listener.train_on_batch(data_batch, states, sample_weight=rewards)\n",
    "                elif train_mode == 'supervised':\n",
    "                        loss = listener.train_on_batch(data_batch, labels_batch)\n",
    "                \n",
    "                rewards_epoch.append(np.mean(rewards))\n",
    "                \n",
    "                states_non_hot = np.argmax(states, axis=1)\n",
    "                correct_states = selection_batch[states_non_hot==selection_batch]\n",
    "                false_states = selection_batch[np.logical_and(states_non_hot!=selection_batch, \n",
    "                                                              has_occurred[states_non_hot]==1)]\n",
    "                potentially_correct_states = selection_batch[np.logical_and(states_non_hot!=selection_batch, \n",
    "                                                                            has_occurred[states_non_hot]==0)]\n",
    "                unique, occurrences = np.unique(correct_states, return_counts=True)\n",
    "                counts[i, unique, 0] += occurrences \n",
    "                unique, occurrences = np.unique(false_states, return_counts=True)\n",
    "                counts[i, unique, 1] += occurrences \n",
    "                unique, occurrences = np.unique(potentially_correct_states, return_counts=True)\n",
    "                counts[i, unique, 2] += occurrences \n",
    "                \n",
    "                has_occurred[selection_batch] = 1\n",
    "            \n",
    "            mean_rewards = np.mean(rewards_epoch)\n",
    "            all_rewards.append(mean_rewards)\n",
    "            \n",
    "        \n",
    "        print('final reward ' + str(all_rewards[iter_step * n - 1]))\n",
    "        \n",
    "        # save results \n",
    "        np.save(filename + 'policies_ref_single_' + str(run), policies_all_reference) # single because of ONE distractor\n",
    "        np.save(filename + 'policies_lewis_' + str(run), policies_all_lewis)\n",
    "        np.save(filename + 'rewards_' + str(run), all_rewards)\n",
    "        np.save(filename + 'counts_' + str(run), counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iter_step in [1,3,6,9,12,15]:\n",
    "    if iter_step == 1: \n",
    "        run_agent(iter_step=iter_step, n_epochs=iter_step*100 + 1000, runs=500)\n",
    "    else:\n",
    "        run_agent(iter_step=iter_step, n_epochs=iter_step*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
