{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential, layers, losses, optimizers, activations, Model, datasets, models\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from helper_functions.calculation_helpers import *\n",
    "from neural_agents import LiteralListener, PragmaticListener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE DATA SET AND LOAD MNIST CLASSIFIER \n",
    "\n",
    "(train_images, train_labels_orig), (test_images, test_labels_orig) = datasets.mnist.load_data()\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "train_images = train_images.reshape(60000, 28, 28, 1)\n",
    "test_images = test_images.reshape(len(test_labels_orig), 28, 28, 1)\n",
    "\n",
    "train, test, ME = np.load('mnist_referential_dataset/train_test_ME.npy', allow_pickle=True)\n",
    "(target_indices, target_labels, distractor_indices, distractor_labels) = train\n",
    "(test_target_indices, test_target_labels, test_distractor_indices, test_distractor_labels) = test\n",
    "(ME_indices, ME_labels) = ME\n",
    "\n",
    "n_train = len(target_labels)\n",
    "\n",
    "mnist_classifier = models.load_model('mnist_classifier/model.06-0.04.h5')\n",
    "feature_extraction_model = tf.keras.Model(inputs=mnist_classifier.input,\n",
    "                                          outputs=mnist_classifier.get_layer('dense').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION FOR TRAINING THE NEURAL AGENTS\n",
    "\n",
    "def run_agent(vocab_size=20,\n",
    "              activation='sigmoid',\n",
    "              agent_type='pragmatic',\n",
    "              alpha = 5.,\n",
    "              runs = 25,\n",
    "              negative_sampling=None, # None, 'words', 'objects', or 'both',\n",
    "              message_length = 1,\n",
    "              encoding_dim = 32,\n",
    "              n_distractors = 1,\n",
    "              n_classes = 10,\n",
    "              n_training = n_train,\n",
    "              n_epochs = 100,\n",
    "              batch_size = 64,\n",
    "              learning_rate = 0.0001\n",
    "             ):\n",
    "    \n",
    "    n_batches = n_training // batch_size\n",
    "    if agent_type == 'pragmatic':\n",
    "        path_ending = '_alpha_' + str(alpha) + '/'\n",
    "    else: \n",
    "        path_ending = '/'\n",
    "\n",
    "    # create messages as integers 0-19\n",
    "    all_messages = np.zeros((vocab_size,message_length))\n",
    "    for i in range(vocab_size):\n",
    "        all_messages[i] = i\n",
    "    \n",
    "    ### iterate over runs \n",
    "    for run in range(runs):\n",
    "        \n",
    "        print('run', run)\n",
    "        \n",
    "        # randomly assign one message to each digit 0-8\n",
    "        np.random.shuffle(all_messages)\n",
    "        message_code = all_messages[:9]\n",
    "        ME_message_code = all_messages[9:]\n",
    "        \n",
    "        messages = np.zeros((n_train, message_length))\n",
    "        for i in range(9):\n",
    "            messages[target_labels==i] = message_code[i]\n",
    "        \n",
    "        \n",
    "        ### set paths for different negative sampling strategies \n",
    "        \n",
    "        if negative_sampling == 'both' or negative_sampling == 'objects':\n",
    "            folder = ('results/' + agent_type + '_negative_sampling_' + \n",
    "                      str(negative_sampling) + path_ending + 'run' + str(run) + '/')\n",
    "            \n",
    "            # add examples of digit 9 to the distractors if sampling is 'both' or 'objects'\n",
    "            replacement_indices = np.where(np.random.binomial(1, 1/10, size=len(distractor_indices))==1)[0]\n",
    "            if len(replacement_indices) > len(ME_indices):\n",
    "                replacement_indices = replacement_indices[:len(ME_indices)]\n",
    "            random_ME_indices = np.random.choice(len(ME_indices), size=len(replacement_indices), replace=False)\n",
    "            distractor_indices[replacement_indices] = ME_indices[random_ME_indices]\n",
    "            distractor_labels[replacement_indices] = ME_labels[random_ME_indices]\n",
    "        \n",
    "        elif negative_sampling == 'words':\n",
    "            folder = ('results/' + agent_type + '_negative_sampling_words' + path_ending + 'run' + str(run) + '/')\n",
    "        \n",
    "        elif negative_sampling is None:\n",
    "            folder = ('results/' + agent_type + '_no_negative_sampling' + path_ending + 'run' + str(run) + '/')\n",
    "        \n",
    "        \n",
    "        ### save the parameters \n",
    "        \n",
    "        param_dict = {\"VOCAB_SIZE\": vocab_size, \n",
    "                      \"MESSAGE_LENGTH\": message_length, \n",
    "                      \"ALPHA\": alpha, \n",
    "                      \"ENCODING_DIM\": encoding_dim, \n",
    "                      \"N_DISTRACTORS\": n_distractors, \n",
    "                      \"N_CLASSES\" : n_classes, \n",
    "                      \"N_TRAINING\": n_training, \n",
    "                      \"N_EPOCHS\": n_epochs, \n",
    "                      \"BATCH_SIZE\": batch_size, \n",
    "                      \"N_EPOCHS\": n_epochs, \n",
    "                      \"MESSAGE_CODE\": message_code, \n",
    "                      \"ME_MESSAGE_CODE\": ME_message_code, \n",
    "                      \"NEGATIVE_SAMPLING\": negative_sampling, \n",
    "                      \"STATE_ENCODER\": 'one_layer_' + activation, \n",
    "                      \"MESSAGE_ENCODER\": 'one_layer_' + activation}\n",
    "        \n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        with open(folder + 'param_dict.pickle', 'wb') as handle:\n",
    "            pickle.dump(param_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        \n",
    "        ### build the agent \n",
    "        \n",
    "        message_encoder = Sequential(\n",
    "            [layers.Embedding(input_dim=vocab_size, output_dim=32, \n",
    "                              input_length=message_length), \n",
    "             layers.Flatten(),\n",
    "             layers.Dense(encoding_dim, activation=activation)], \n",
    "            name='message_encoder'\n",
    "        )\n",
    "        \n",
    "        print(message_encoder.summary())\n",
    "        \n",
    "        state_encoder = Sequential(\n",
    "            [layers.Dense(encoding_dim, activation=activation)], \n",
    "            name='state_encoder'\n",
    "        )\n",
    "        \n",
    "        if agent_type == 'literal':\n",
    "            agent = LiteralListener(vocab_size = vocab_size, \n",
    "                                    message_length = message_length,\n",
    "                                    encoding_dim = encoding_dim,\n",
    "                                    n_distractors = n_distractors,\n",
    "                                    messages = message_code,\n",
    "                                    state_encoder = state_encoder, \n",
    "                                    message_encoder= message_encoder\n",
    "                                   )\n",
    "        elif agent_type == 'pragmatic':\n",
    "            agent = PragmaticListener(alpha=alpha, \n",
    "                                      vocab_size = vocab_size, \n",
    "                                      message_length = message_length,\n",
    "                                      encoding_dim = encoding_dim,\n",
    "                                      n_distractors = n_distractors,\n",
    "                                      messages = message_code,\n",
    "                                      state_encoder = state_encoder, \n",
    "                                      message_encoder= message_encoder\n",
    "                                     )\n",
    "            \n",
    "        agent.build(vision_dim=64)\n",
    "        agent_optim = optimizers.Adam(lr=learning_rate)\n",
    "        agent_loss = losses.CategoricalCrossentropy()\n",
    "        \n",
    "        \n",
    "        ### training\n",
    "        \n",
    "        # store rewards, losses and ME evlauation results per epoch\n",
    "        all_rewards, all_losses, ME_evaluation = [], [], []\n",
    "        \n",
    "        # create tensorflow Dataset from data\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((feature_extraction_model(train_images[target_indices]),\n",
    "                                                        feature_extraction_model(train_images[distractor_indices]),\n",
    "                                                        messages, target_labels, distractor_labels))\n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        \n",
    "        # training loop\n",
    "        for epoch in range(n_epochs): \n",
    "            print('epoch', epoch)\n",
    "            \n",
    "            train_dataset = train_dataset.shuffle(buffer_size=n_train)\n",
    "            train_iterator = iter(train_dataset)\n",
    "        \n",
    "            collect_rewards, collect_losses = [], []\n",
    "        \n",
    "            for batch in range(n_batches):\n",
    "                \n",
    "                targets, distractors, messages, _, _ = train_iterator.get_next()\n",
    "                labels = np.stack([np.ones(len(messages)), np.zeros(len(messages))], axis=1)         \n",
    "                      \n",
    "                if negative_sampling == 'both' or negative_sampling == 'words':\n",
    "                    message_proposals = np.reshape(np.tile(all_messages, [len(targets), 1]), \n",
    "                                                   (len(targets), len(all_messages), message_length))\n",
    "                elif negative_sampling == 'objects' or negative_sampling is None:\n",
    "                    message_proposals = np.reshape(np.tile(message_code, [len(targets), 1]), \n",
    "                                                   (len(targets), len(message_code), message_length))\n",
    "                \n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    tape.watch(agent.trainable_variables)\n",
    "                    \n",
    "                    selections, log_policy = agent.listener_action([messages, \n",
    "                                                                    targets, \n",
    "                                                                    [distractors],  \n",
    "                                                                    message_proposals, \n",
    "                                                                    [message_proposals]], \n",
    "                                                                    expand_messages = False)   \n",
    "                    \n",
    "                    rewards = np.array(tf.cast(tf.equal(labels, selections), tf.float32)[:,0])\n",
    "                    \n",
    "                    policy = tf.math.exp(log_policy)      \n",
    "                    loss = - tf.reduce_mean(selections * log_policy * tf.expand_dims(rewards, axis=1))\n",
    "        \n",
    "                \n",
    "                grads = tape.gradient(loss, agent.trainable_variables)\n",
    "                agent_optim.apply_gradients(zip(grads, agent.trainable_variables))\n",
    "                \n",
    "                collect_rewards.append(np.mean(rewards))\n",
    "                collect_losses.append(loss)\n",
    "            \n",
    "            all_rewards.append(np.mean(collect_rewards))\n",
    "            all_losses.append(np.mean(collect_losses))\n",
    "            \n",
    "            \n",
    "            ### ME bias evaluation\n",
    "        \n",
    "            n_ME = len(ME_indices)\n",
    "            \n",
    "            eval_messages = np.zeros((n_ME, message_length))\n",
    "            shuffled_indices = np.random.choice(len(ME_message_code), replace=True, size=n_ME)\n",
    "            for i in range(n_ME):\n",
    "                eval_messages[i] = ME_message_code[shuffled_indices[i]]\n",
    "            \n",
    "            eval_targets = feature_extraction_model(train_images[ME_indices])\n",
    "            eval_distractors = feature_extraction_model(test_images[0:n_ME])\n",
    "            \n",
    "            labels = np.stack([np.ones(len(eval_messages)), np.zeros(len(eval_messages))], axis=1)\n",
    "            \n",
    "            if negative_sampling == 'both' or negative_sampling == 'words': \n",
    "                message_proposals = np.reshape(np.tile(all_messages, [len(eval_targets), 1]), \n",
    "                                               (len(eval_targets), len(all_messages), message_length))\n",
    "                \n",
    "                selections, log_policy = agent.listener_action([eval_messages, \n",
    "                                                                eval_targets, \n",
    "                                                                [eval_distractors],  \n",
    "                                                                message_proposals, \n",
    "                                                                [message_proposals]], \n",
    "                                                               expand_messages=False) \n",
    "                \n",
    "            elif negative_sampling == 'objects' or negative_sampling is None: \n",
    "                message_proposals = np.reshape(np.tile(message_code, [len(eval_targets), 1]), \n",
    "                                               (len(eval_targets), len(message_code), message_length))\n",
    "                \n",
    "                selections, log_policy = agent.listener_action([eval_messages, \n",
    "                                                                eval_targets, \n",
    "                                                                [eval_distractors],  \n",
    "                                                                message_proposals, \n",
    "                                                                [message_proposals]], \n",
    "                                                               expand_messages=True)           \n",
    "                \n",
    "            rewards = np.array(tf.cast(tf.equal(labels, selections), tf.float32)[:,0])\n",
    "            ME_eval = np.mean(rewards)\n",
    "            ME_evaluation.append(ME_eval)\n",
    "        \n",
    "        print('... saving models ...')\n",
    "        np.save(folder + 'state_encoder_weights', agent.state_encoder.get_weights())\n",
    "        np.save(folder + 'state_encoder_config', agent.state_encoder.get_config())\n",
    "        np.save(folder + 'message_encoder_weights', agent.message_encoder.get_weights())\n",
    "        np.save(folder + 'message_encoder_config', agent.message_encoder.get_config())\n",
    "        print('... models saved ...')\n",
    "        \n",
    "        \n",
    "        ### evaluation on the test data \n",
    "        \n",
    "        eval_targets = feature_extraction_model(test_images[test_target_indices])\n",
    "        eval_distractors = feature_extraction_model(test_images[test_distractor_indices])\n",
    "        n_test = len(test_target_labels)\n",
    "        \n",
    "        eval_messages = np.zeros((n_test, message_length))\n",
    "        for i in range(9):\n",
    "            eval_messages[test_target_labels==i] = message_code[i]\n",
    "        \n",
    "        labels = np.stack([np.ones(len(eval_messages)), np.zeros(len(eval_messages))], axis=1)\n",
    "        \n",
    "        if negative_sampling: \n",
    "            message_proposals = np.reshape(np.tile(all_messages, [len(eval_targets), 1]), \n",
    "                                           (len(eval_targets), len(all_messages), message_length))\n",
    "        else: \n",
    "            message_proposals = np.reshape(np.tile(message_code, [len(eval_targets), 1]), \n",
    "                                           (len(eval_targets), len(message_code), message_length))\n",
    "                      \n",
    "        selections, log_policy = agent.listener_action([eval_messages, \n",
    "                                                        eval_targets, \n",
    "                                                        [eval_distractors],  \n",
    "                                                        message_proposals, \n",
    "                                                        [message_proposals]], \n",
    "                                                       expand_messages=False)          \n",
    "        \n",
    "        rewards = np.array(tf.cast(tf.equal(labels, selections), tf.float32)[:,0])\n",
    "        test_eval = np.mean(rewards)\n",
    "        \n",
    "        print('test performance: ', test_eval, 'ME bias: ', ME_evaluation[-1])\n",
    "        \n",
    "        ### save run\n",
    "        \n",
    "        np.save(folder + 'rewards', all_rewards)\n",
    "        np.save(folder + 'losses', all_losses)\n",
    "        np.save(folder + 'ME', ME_evaluation)\n",
    "        np.save(folder + 'test', test_eval)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN THE AGENTS\n",
    "\n",
    "## run pragmatic agent without negative sampling for different values of alpha\n",
    "for alpha in [5, 10, 15]:\n",
    "    run_agent(alpha=alpha, activation='sigmoid', runs=25) \n",
    "\n",
    "## run pragmatic agent with alpha=5 and different negative sampling strategies\n",
    "for negative_sampling in ['words', 'objects', 'both']:\n",
    "    run_agent(alpha=5, negative_sampling=negative_sampling, activation='sigmoid', runs=25)   \n",
    "\n",
    "## run the literal agent without negative sampling\n",
    "run_agent(agent_type='literal', activation='sigmoid', runs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
